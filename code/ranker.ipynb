{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "mSTyIzplNvvo",
    "outputId": "e270eb8e-7e16-4ffb-e897-b9e36194ad41",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2bhnjP2PP9r"
   },
   "outputs": [],
   "source": [
    "!unzip  \"gdrive/My Drive/result.zip\"  \"result/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-uTvtMAtdcxJ"
   },
   "outputs": [],
   "source": [
    "!pip3 install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "9nX3lsvINfNK",
    "outputId": "49779425-9787-469f-e12b-d02c9303a48e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import codecs\n",
    "import pickle\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "RUSSIANSTOPWORDS = set(stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i9qY6ZKuNfNk"
   },
   "outputs": [],
   "source": [
    "class Lexicon():\n",
    "    def __init__(self):\n",
    "        self.dictionary = {}\n",
    "        self.counter = 0\n",
    "        \n",
    "    \n",
    "    def add(self, word):\n",
    "        if (word in self.dictionary):\n",
    "            return False\n",
    "        \n",
    "        self.dictionary[word] = self.counter\n",
    "        self.counter += 1\n",
    "        return True\n",
    "    \n",
    "    def get_num(self, word):\n",
    "        if word in self.dictionary:\n",
    "            return self.dictionary[word]\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "lex = Lexicon()   \n",
    "        \n",
    "\n",
    "def get_forward_index(dirname):\n",
    "    \n",
    "    doc_id_txt = {}\n",
    "    for idx,filename in enumerate(sorted(os.listdir(dirname))):\n",
    "        if (idx % 1000 == 0):\n",
    "            print(idx)\n",
    "        number = int(filename.split('.')[0])\n",
    "        with codecs.open(os.path.join(dirname, filename),'r','utf-8',errors='replace') as f:\n",
    "            title = f.readline().rstrip()\n",
    "            h = []\n",
    "            for i in range(6):\n",
    "                h.append(f.readline().rstrip())\n",
    "\n",
    "            text = f.readline().rstrip()\n",
    "\n",
    "        doc_id_txt[number] = (title, h, text) \n",
    "\n",
    "    return doc_id_txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "sz4dK3qANfN4",
    "outputId": "5203bbc2-71db-48ce-a938-66d5c48f4208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n"
     ]
    }
   ],
   "source": [
    "forward_index = get_forward_index('result/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nPOmBQANfOa"
   },
   "outputs": [],
   "source": [
    "def create_inverse_index(dirname):\n",
    "    \n",
    "    stats = dict()\n",
    "    inverse_index = dict()\n",
    "    filenames = os.listdir(dirname)\n",
    "    for idx, filename in enumerate(filenames):\n",
    "        if (idx % 1000 ==0):\n",
    "            print(idx)\n",
    "            \n",
    "        docID = int(filename.split('.')[0])\n",
    "        fullpath = os.path.join(dirname, filename)\n",
    "        with codecs.open(fullpath, 'r', 'utf-8', errors='replace') as f:\n",
    "            i = 0\n",
    "            for line in f:\n",
    "                for word in line.split():\n",
    "                    i += 1\n",
    "                    if word in inverse_index:\n",
    "                        inverse_index[word].add(docID)\n",
    "                    else:\n",
    "                        inverse_index[word] = set([docID])\n",
    "        stats[docID] = i\n",
    "    return inverse_index, stats\n",
    "\n",
    "def create_inverse_index2(forward_index):\n",
    "\n",
    "    word_count = dict()\n",
    "    inverse_index = dict()\n",
    "\n",
    "    for idx, docID in enumerate(forward_index.keys()):\n",
    "        if (idx % 1000 == 0):\n",
    "            print(idx)\n",
    "        \n",
    "        title, h, text = forward_index[docID]\n",
    "        all_text = title + ' ' +text\n",
    "        for word in all_text.split():\n",
    "            if (word in inverse_index):\n",
    "                inverse_index[word].add(docID)\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                inverse_index[word] = set([docID])\n",
    "                word_count[word] = 1\n",
    "\n",
    "    return inverse_index, word_count\n",
    "\n",
    "def create_inverse_index_bigrams(forward_index):\n",
    "    bigram_count = dict()\n",
    "    bigram_inverse_index = dict()\n",
    "\n",
    "    for idx, docID in enumerate(forward_index.keys()):\n",
    "        if (idx % 1000 == 0):\n",
    "            print(idx)\n",
    "        \n",
    "        title, h, text = forward_index[docID]\n",
    "        all_text = (title + ' ' +text).split()\n",
    "        text_bigrams =  ['{} {}'.format(all_text[i], all_text[i+1]) \n",
    "                         for i in range(0, len(all_text)-1, 2)]\n",
    "\n",
    "        for bigram in text_bigrams:\n",
    "            if (bigram in bigram_inverse_index):\n",
    "                bigram_inverse_index[bigram].add(docID)\n",
    "                bigram_count[bigram] += 1\n",
    "            else:\n",
    "                bigram_inverse_index[bigram] = set([docID])\n",
    "                bigram_count[bigram] = 1\n",
    "\n",
    "    return bigram_inverse_index, bigram_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "colab_type": "code",
    "id": "B84WT0xTNfOp",
    "outputId": "20e813c4-d238-4038-ba71-71f0c24d8759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n"
     ]
    }
   ],
   "source": [
    "inverse_index, word_count = create_inverse_index2(forward_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDoAah3IhhGp"
   },
   "outputs": [],
   "source": [
    "len(inverse_index['казань'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqjRCGDrNfPB"
   },
   "outputs": [],
   "source": [
    "with open(\"inverse_index.pickle\", 'wb') as f:\n",
    "    pickle.dump(inverse_index,f)\n",
    "\n",
    "with open('word_count.pickle','wb') as f:\n",
    "    pickle.dump(word_count, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5E36iA1f5pCt"
   },
   "outputs": [],
   "source": [
    "with open(\"forward_index.pickle\", 'wb') as f:\n",
    "    pickle.dump(forward_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WH7ElVifNfPO"
   },
   "outputs": [],
   "source": [
    "with open(\"inverse_index.pickle\", 'rb') as f:\n",
    "    inverse_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nr3XLGaeNfPa"
   },
   "outputs": [],
   "source": [
    "with open('word_count.pickle','rb') as f:\n",
    "    word_count = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4d0CURDo_SPq"
   },
   "outputs": [],
   "source": [
    "with open('forward_index.pickle','rb') as f:\n",
    "    forward_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b8DPxPMXNfP2"
   },
   "outputs": [],
   "source": [
    "#simple bm25\n",
    "class BM25:\n",
    "\n",
    "    def __init__(self, inverse_index, word_count, forward_index):\n",
    "        self.inverse_index = inverse_index\n",
    "        self.word_count = word_count\n",
    "        #self.avgdl = sum([doc_stats[doc_id] for doc_id in doc_stats]) / len(doc_stats)\n",
    "        self.forward_index = forward_index\n",
    "        self.docs_total = len(self.forward_index)\n",
    "        self.total_words = sum([word_count[key] for key in word_count.keys()])\n",
    "\n",
    "    def get_important(self, words, words_idf, threshold = 0.9):\n",
    "\n",
    "\n",
    "        if (len(words) <=  1):\n",
    "            return words\n",
    "\n",
    "        \n",
    "        variants = [words]\n",
    "\n",
    "        for variant in variants:\n",
    "            total_idf = sum([words_idf[word] for word in variant])\n",
    "            for idx, word in enumerate(variant):\n",
    "                probable = variant[:idx] + variant[idx+1:]\n",
    "                if ((total_idf - words_idf[word]) / total_idf > threshold and probable not in variants):\n",
    "                    variants.append(probable)\n",
    "                    \n",
    "        return variants\n",
    "         \n",
    "    def computeScore(self, words, text, words_idf):\n",
    "        def find_positions(words, text):\n",
    "\n",
    "            wordset = set(words)\n",
    "            words_positions = {word:[] for word in words}\n",
    "            for idx, text_word in enumerate(text):\n",
    "                if text_word in wordset:\n",
    "                    words_positions[text_word].append(idx)\n",
    "\n",
    "            return words_positions\n",
    "        \n",
    "\n",
    "        text = text.split()\n",
    "        words_positions = find_positions(words, text)\n",
    "        #print(words_positions)\n",
    "        Nmiss = sum([1 for word in words_positions.keys() \n",
    "                        if not len(words_positions[word])])\n",
    "\n",
    "        words = [word for word in words_positions.keys() if len(words_positions[word]) > 0]\n",
    "        single_score = 0\n",
    "        ''' \n",
    "        for idx, word in enumerate(words):\n",
    "            word_idf = words_idf[word]\n",
    "            word_freq = len(words_positions[word])\n",
    "            word_score = 0\n",
    "            for position in words_positions[word]:\n",
    "                word_score += 1 / (1 + np.log(position + 1)) * word_idf\n",
    "\n",
    "            word_score /=  (len(words_positions[word]) + 1e-6)\n",
    "            single_score += word_score\n",
    "        \n",
    "        '''\n",
    "        pair_score = 0\n",
    "        for offset in range(0,2):\n",
    "            \n",
    "            for idx in range(len(words) - 1 - offset):\n",
    "                prev_word = words[idx]\n",
    "                next_word = words[idx + 1 + offset]\n",
    "                if (prev_word == next_word) : continue\n",
    "                pair_tf = 0\n",
    "                i = 0\n",
    "                j = 0\n",
    "                pairs = 0\n",
    "                while i < len(words_positions[prev_word]) and j < len(words_positions[next_word]):\n",
    "                    prev_pos = words_positions[prev_word][i]\n",
    "                    next_pos = words_positions[next_word][j]\n",
    "                    #print(\"pos:\", prev_pos, next_pos)\n",
    "                    if abs(prev_pos - next_pos) < 3:\n",
    "                        pair_tf +=  1 /abs(prev_pos - next_pos)\n",
    "                        i += 1\n",
    "                        j += 1\n",
    "\n",
    "                    elif (prev_pos > next_pos):\n",
    "                        j += 1\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "                pair_score +=  0.2 * (min(words_idf[prev_word], words_idf[next_word])) * pair_tf/ (1 + pair_tf)\n",
    "        \n",
    "        all_words_score = 0\n",
    "        for word in words:\n",
    "           all_words_score += 0.1 * words_idf[word] * (0.03 ** Nmiss)\n",
    "\n",
    "\n",
    "        #print(\"words:\", words)\n",
    "        #print(\"single: {}  all: {}\".format(single_score, all_words_score))\n",
    "\n",
    "        score = pair_score + all_words_score\n",
    "\n",
    "        return score \n",
    "\n",
    "\n",
    "    def __call__(self, query, q_id):\n",
    "\n",
    "\n",
    "        splitted = query.split()\n",
    "        words_idf = {}\n",
    "        words = []\n",
    "\n",
    "        for word in splitted:\n",
    "            if word in inverse_index:\n",
    "                words.append(word)\n",
    "                words_idf[word] = np.log(self.total_words / len(inverse_index[word]))\n",
    "\n",
    "        all_docs = set()\n",
    "        threshold = 0.7\n",
    "\n",
    "        if (len(words) > 12):\n",
    "            wordidf = [(word, words_idf[word]) for word in words]\n",
    "            wordidf = sorted(wordidf, key=lambda x:x[1], reverse=True)[:12]\n",
    "            wordidf = [word[0] for word in wordidf]\n",
    "            words = [word for word in words if word in wordidf]\n",
    "\n",
    "        while (len(all_docs) < 10):\n",
    "            \n",
    "                print(threshold)\n",
    "                #print(len(all_docs))\n",
    "                probable_queries = self.get_important(words, words_idf, threshold)\n",
    "                #print(probable_queries)\n",
    "                for probable_query in probable_queries:\n",
    "\n",
    "                    relevant = self.inverse_index[probable_query[0]]\n",
    "                    for word in probable_query[1:]:\n",
    "                        \n",
    "                        relevant = relevant.intersection(self.inverse_index[word])\n",
    "                    \n",
    "                    all_docs |= relevant\n",
    "                \n",
    "                #print(\"ALL\", len(all_docs))\n",
    "                threshold -= 0.1\n",
    "            \n",
    "        #print(all_docs)\n",
    "                all_docs = set(\n",
    "                        [docID for docID in all_docs \n",
    "                        if (docID > 96*q_id - 300) and (docID < 96*q_id + 300)]\n",
    "                        )\n",
    "        docID_score = []\n",
    "\n",
    "        title_corpus = []\n",
    "        text_corpus_unigrams = []    \n",
    "        text_corpus_bigrams = []\n",
    "        docIDs = [docID for docID in all_docs]\n",
    "        my_score = []\n",
    "        \n",
    "        for docID in docIDs:\n",
    "            \n",
    "            \n",
    "            title, h, text = self.forward_index[docID]\n",
    "\n",
    "            title_corpus.append(title.split())\n",
    "            splitted_text = text.split()\n",
    "            text_corpus_unigrams.append(splitted_text)\n",
    "            text_corpus_bigrams.append([' '.join(splitted_text[i:i+2]) \n",
    "                                for i in range(0, len(splitted_text), 2)]\n",
    "                                ) \n",
    "            my_score.append(self.computeScore(words, self.forward_index[docID][-1], words_idf))\n",
    "        \n",
    "        bm25_text_bigrams = BM25Okapi(text_corpus_bigrams)\n",
    "\n",
    "        bm25_title = BM25Okapi(title_corpus)\n",
    "        title_scores = bm25_title.get_scores(words)\n",
    "\n",
    "        bm25_text_unigrams = BM25Okapi(text_corpus_unigrams)\n",
    "        text_scores_unigrams = bm25_text_unigrams.get_scores(words)\n",
    "\n",
    "\n",
    "        total_score = [tscore + txtscore\n",
    "                        for tscore, txtscore, myscore in zip(title_scores, text_scores_unigrams, my_score)]\n",
    "        docID_score = sorted([(docID, score) for docID, score in zip(docIDs, total_score)], key=lambda x:x[1], reverse=True)\n",
    "        return [docID_score[i][0] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "-ECWepHaag3X",
    "outputId": "52e5e610-f274-4ffd-92c0-c3ecd9a7c456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10551, 10568, 10591, 10529, 10597, 10613, 10544, 10588, 10550, 10579]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doc_id_text.keys()[:10]\n",
    "\n",
    "ranger = BM25(inverse_index, word_count, forward_index)\n",
    "ranger(\"где можно постирать ковер\", 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BrBsrq0yfoHe"
   },
   "outputs": [],
   "source": [
    "#create 4 workers for ranking\n",
    "import multiprocessing \n",
    "\n",
    "def worker(job_id):\n",
    "    ranger = BM25(inverse_index, word_count, doc_id_text)\n",
    "    with open('./gdrive/My Drive/queries.txt', 'r') as fr:\n",
    "        with open('./{}result.txt'.format(job_id // 100) ,'w') as fw: \n",
    "            fw.write(\"QueryId,DocumentId\\n\")\n",
    "            for idx, line in enumerate(fr):\n",
    "                if not(idx >= job_id and job_id + 100 > idx) : continue\n",
    "                \n",
    "\n",
    "                query_id, query = int(line.split()[0]), ' '.join(line.split()[1:])\n",
    "                try:\n",
    "                    result = ranger(query, query_id)\n",
    "                except:\n",
    "                    print(\"exception occured in \", query_id)\n",
    "\n",
    "                for docID in result:\n",
    "                    fw.write(\"{},{}\\n\".format(query_id, docID))\n",
    "\n",
    "\n",
    "\n",
    "processes = []\n",
    "for i in range(0, 400, 100):\n",
    "    p = multiprocessing.Process(target=worker, args=(i,))\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "    \n",
    "for process in processes:\n",
    "    process.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35zKzBGvgAof"
   },
   "outputs": [],
   "source": [
    "def unite():\n",
    "    with open(\"final.csv\", 'w') as f:\n",
    "        f.write(\"QueryId,DocumentId\\n\")\n",
    "        for i in range(4):\n",
    "            with open(\"{}result.txt\".format(i), 'r') as rf:\n",
    "                next(rf)\n",
    "                for line in rf:\n",
    "                    f.write(line)\n",
    "unite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UK5s4hWsipRZ"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "R6GmeC0uHTUr",
    "outputId": "a9a2fb96-a52c-4451-ebd1-28b4e9ef8da7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-15 10:32:25--  http://vectors.nlpl.eu/repository/11/196.zip\n",
      "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
      "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 206986345 (197M) [application/zip]\n",
      "Saving to: ‘196.zip’\n",
      "\n",
      "196.zip             100%[===================>] 197.40M  9.86MB/s    in 27s     \n",
      "\n",
      "2019-12-15 10:32:54 (7.26 MB/s) - ‘196.zip’ saved [206986345/206986345]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://vectors.nlpl.eu/repository/11/196.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JkU1mZNEw5R8",
    "outputId": "a215c790-5015-4dd7-d44b-cafeacff897b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 737kB 5.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 25.2MB 159kB/s \n",
      "\u001b[K     |████████████████████████████████| 317kB 45.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 10.1MB 37.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 8.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 5.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 6.7MB 17.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.1MB 29.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 8.0MB 15.7MB/s \n",
      "\u001b[K     |████████████████████████████████| 163kB 47.7MB/s \n",
      "\u001b[K     |████████████████████████████████| 17.3MB 198kB/s \n",
      "\u001b[K     |████████████████████████████████| 2.8MB 39.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.3MB 49.4MB/s \n",
      "\u001b[K     |████████████████████████████████| 7.1MB 17.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 8.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 81kB 11.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 112kB 52.6MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.9MB 35.7MB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 5.1MB 33.4MB/s \n",
      "\u001b[?25h  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for uvicorn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for httptools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: plotnine 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mizani 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.25.0; python_version >= \"3.0\", but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "email-validator not installed, email fields will be treated as str.\n",
      "To install, run: pip install email-validator\n",
      "2020-01-11 16:31:44.824 INFO in 'deeppavlov.core.common.file'['file'] at line 30: Interpreting 'squad_bert' as '/usr/local/lib/python3.6/dist-packages/deeppavlov/configs/squad/squad_bert.json'\n",
      "Collecting tensorflow==1.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
      "\u001b[K     |████████████████████████████████| 109.2MB 35kB/s \n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.9.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.33.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.1.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "\u001b[K     |████████████████████████████████| 491kB 40.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.11.2)\n",
      "Collecting tensorboard<1.15.0,>=1.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 37.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.16.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (42.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (0.16.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.9.0)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
      "  Found existing installation: tensorflow-estimator 1.15.1\n",
      "    Uninstalling tensorflow-estimator-1.15.1:\n",
      "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
      "  Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "  Found existing installation: tensorflow 1.15.0\n",
      "    Uninstalling tensorflow-1.15.0:\n",
      "      Successfully uninstalled tensorflow-1.15.0\n",
      "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n",
      "Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
      "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-kmzfnmah\n",
      "  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-kmzfnmah\n",
      "  Running command git checkout -b feat/multi_gpu --track origin/feat/multi_gpu\n",
      "  Switched to a new branch 'feat/multi_gpu'\n",
      "  Branch 'feat/multi_gpu' set up to track remote branch 'feat/multi_gpu' from 'origin'.\n",
      "Building wheels for collected packages: bert-dp\n",
      "  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for bert-dp: filename=bert_dp-1.0-cp36-none-any.whl size=23580 sha256=e9b9acca763da8b7e2bbcea04ca925008ef159456867f5bb7ac3b2227f7939b1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-53_kmnj9/wheels/1e/41/94/886107eaf932532594886fd8bfc9cb9d4db632e94add49d326\n",
      "Successfully built bert-dp\n",
      "Installing collected packages: bert-dp\n",
      "Successfully installed bert-dp-1.0\n",
      "email-validator not installed, email fields will be treated as str.\n",
      "To install, run: pip install email-validator\n",
      "2020-01-11 16:32:53.468 INFO in 'deeppavlov.core.common.file'['file'] at line 30: Interpreting 'ru_odqa_infer_wiki' as '/usr/local/lib/python3.6/dist-packages/deeppavlov/configs/odqa/ru_odqa_infer_wiki.json'\n",
      "Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.10.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.33.6)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.1.8)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.9.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.11.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.16.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (42.0.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (0.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.1.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q deeppavlov\n",
    "!python3 -m deeppavlov install squad_bert\n",
    "!python3 -m deeppavlov install ru_odqa_infer_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHv2DPWpqq6P"
   },
   "outputs": [],
   "source": [
    "#BERT Model for Ranking\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "from deeppavlov import configs\n",
    "from deeppavlov.core.common.file import read_json\n",
    "from deeppavlov import  train_model, build_model\n",
    "\n",
    "class PavlovRanker:\n",
    "\n",
    "    def __init__(self, inverse_index, word_count, forward_index, BertModel):\n",
    "\n",
    "        #------for computing idf-----\n",
    "        self.inverse_index = inverse_index\n",
    "        self.word_count = word_count\n",
    "        #----------------------------------\n",
    "\n",
    "        self.forward_index = forward_index\n",
    "\n",
    "\n",
    "        self.docs_total = len(self.forward_index)\n",
    "        self.total_words = sum([word_count[key] for key in word_count.keys()])\n",
    "\n",
    "        self.model = BertModel \n",
    "\n",
    "    \n",
    "    def computeScore(self, words, text, words_idf, avgdl):\n",
    "        '''\n",
    "        Classic bm25okapi scorer function with position bonus\n",
    "        '''\n",
    "\n",
    "        #-----addition function for searching word's positions------------------\n",
    "        def find_positions(words, text):\n",
    "\n",
    "            wordset = set(words)\n",
    "            words_positions = {word:[] for word in words}\n",
    "            for idx, text_word in enumerate(text):\n",
    "                if text_word in wordset:\n",
    "                    words_positions[text_word].append(idx)\n",
    "\n",
    "            return words_positions\n",
    "        \n",
    "        #-----------------------------------------------------------------------\n",
    "        k1 = 2.0\n",
    "        b = 0.75\n",
    "        #-----------------------------------------------------------------------\n",
    "        text = text.split()\n",
    "        words_positions = find_positions(words, text)\n",
    "        Nmiss = sum([1 for word in words_positions.keys() \n",
    "                        if not len(words_positions[word])])\n",
    "\n",
    "        words = [word for word in words_positions.keys() if len(words_positions[word]) > 0]\n",
    "        single_score = 0\n",
    "        \n",
    "        #----------------------Single score computation-------------------------\n",
    "        for idx, word in enumerate(words):\n",
    "            word_idf = words_idf[word]\n",
    "            word_freq = len(words_positions[word])\n",
    "            tf = len(words_positions[word])\n",
    "\n",
    "            word_score = word_idf * tf * (1 + k1) / (tf + k1*(1-b + b *len(text)/avgdl))\n",
    "\n",
    "            position_score = 0\n",
    "\n",
    "            for position in words_positions[word]:\n",
    "                position_score +=  word_idf * (1 - position_score/len(text))\n",
    "\n",
    "            position_score /= len(words_positions[word])\n",
    "            word_score += position_score\n",
    "            single_score += word_score\n",
    "        \n",
    "        #------------------------Pair score computation-------------------------\n",
    "        pair_score = 0\n",
    "        for offset in range(0,2):\n",
    "            \n",
    "            for idx in range(len(words) - 1 - offset):\n",
    "                prev_word = words[idx]\n",
    "                next_word = words[idx + 1 + offset]\n",
    "                if (prev_word == next_word) : continue\n",
    "                pair_tf = 0\n",
    "                i = 0\n",
    "                j = 0\n",
    "                pairs = 0\n",
    "                while i < len(words_positions[prev_word]) and j < len(words_positions[next_word]):\n",
    "                    prev_pos = words_positions[prev_word][i]\n",
    "                    next_pos = words_positions[next_word][j]\n",
    "                    if abs(prev_pos - next_pos) < 3:\n",
    "                        pair_tf +=  1 /abs(prev_pos - next_pos)\n",
    "                        i += 1\n",
    "                        j += 1\n",
    "\n",
    "                    elif (prev_pos > next_pos):\n",
    "                        j += 1\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "                pair_score +=  0.2 * (min(words_idf[prev_word], words_idf[next_word])) * pair_tf/ (1 + pair_tf)\n",
    "\n",
    "\n",
    "        #-------------Additional Bonus for all word's presence------------------\n",
    "        all_words_score = 0\n",
    "        for word in words:\n",
    "           all_words_score += 0.1 * words_idf[word] * (0.03 ** Nmiss)\n",
    "        #-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        score = single_score + pair_score + all_words_score\n",
    "\n",
    "        return score \n",
    "\n",
    "    \n",
    "    def __call__(self, query, q_id):\n",
    "\n",
    "\n",
    "        splitted = query.split()\n",
    "        words_idf = {}\n",
    "        words = []\n",
    "\n",
    "        for word in splitted:\n",
    "            if word in inverse_index:\n",
    "                words.append(word)\n",
    "                words_idf[word] = np.log(self.total_words / len(inverse_index[word]))\n",
    "\n",
    "            \n",
    "    \n",
    "        all_docs = [i for i in range(96*q_id - 100, 96*q_id + 50, 1)]\n",
    "\n",
    "        avg_dl_text = sum([len(forward_index[doc_id][-1].split()) for doc_id in all_docs]) / len(all_docs)\n",
    "        avg_dl_title = sum([len(forward_index[doc_id][0].split()) for doc_id in all_docs]) / len(all_docs)\n",
    "\n",
    "        docID_score = []\n",
    "\n",
    "        for idx, docID in enumerate(all_docs):\n",
    "            \n",
    "            title = self.forward_index[docID][0]\n",
    "            text = self.forward_index[docID][-1]\n",
    "\n",
    "            bm25_title_score = self.computeScore(words, title, words_idf, avg_dl_title) \n",
    "            bm25_text_score = self.computeScore(words,text, words_idf, avg_dl_text)\n",
    "            answer = self.model([text], query)\n",
    "            answer_score = answer[-1][0]\n",
    "            print(idx, bm25_title_score, bm25_text_score, answer_score)\n",
    "            total_score = 5 * bm25_title_score + bm25_title_score + 3 * answer_score\n",
    "            docID_score.append((docID, total_score))\n",
    "\n",
    "\n",
    "        top10 = sorted(docID_score, key=lambda x:x[1], reverse=True)[:10]\n",
    "        docIDs = [word_score[0] for word_score in top10]\n",
    "        return docIDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "kTFGXmbRs-Qc",
    "outputId": "f1b67155-3c54-43bc-da83-55233f96a227"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-11 16:33:57.38 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/deeppavlov_data/squad_ru_bert.tar.gz to /root/.deeppavlov/squad_ru_bert.tar.gz\n",
      "100%|██████████| 660M/660M [02:59<00:00, 3.67MB/s]\n",
      "2020-01-11 16:36:56.933 INFO in 'deeppavlov.core.data.utils'['utils'] at line 237: Extracting /root/.deeppavlov/squad_ru_bert.tar.gz archive into /root/.deeppavlov/models\n",
      "2020-01-11 16:37:04.510 INFO in 'deeppavlov.core.data.utils'['utils'] at line 80: Downloading from http://files.deeppavlov.ai/deeppavlov_data/bert/multi_cased_L-12_H-768_A-12.zip to /root/.deeppavlov/downloads/multi_cased_L-12_H-768_A-12.zip\n",
      "100%|██████████| 663M/663M [03:05<00:00, 3.58MB/s]\n",
      "2020-01-11 16:40:09.623 INFO in 'deeppavlov.core.data.utils'['utils'] at line 237: Extracting /root/.deeppavlov/downloads/multi_cased_L-12_H-768_A-12.zip archive into /root/.deeppavlov/downloads/bert_models\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
      "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_squad.py:81: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_squad.py:178: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_squad.py:166: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:234: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_squad.py:94: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-11 16:40:50.17 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/squad_ru_bert/model_multi]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/squad_ru_bert/model_multi\n"
     ]
    }
   ],
   "source": [
    "model_config = read_json(configs.squad.squad_ru_bert_infer)\n",
    "model = build_model(model_config, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ezi7I0AxxZAM"
   },
   "outputs": [],
   "source": [
    "pr = PavlovRanker(inverse_index, word_count, forward_index, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JVOdm1aSLKel",
    "outputId": "e8912663-2377-4a03-9f17-20039eb5f65b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 28.072119732881433 0.22750252485275269\n",
      "1 0 90.18403880226901 9.907266616821289\n",
      "2 0 56.435155232343476 7.249207496643066\n",
      "3 45.12429835167505 90.89101751730858 0.6031476855278015\n",
      "4 0 61.987257762712545 6.002791404724121\n",
      "5 18.96857141452895 67.14842943243703 0.061496444046497345\n",
      "6 14.902390629974294 65.43424557158836 11.904730796813965\n",
      "7 21.16592543426699 68.41056966116327 2.044391632080078\n",
      "8 0 67.02291567955429 3.9562740325927734\n",
      "9 0 66.3179967743185 0.02281741425395012\n",
      "10 0 85.14364964010348 6.519864082336426\n",
      "11 0 92.94245130384961 0.710308849811554\n",
      "12 20.32818640773533 69.87849509820217 0.19885209202766418\n",
      "13 0 92.2080903771893 0.06445620208978653\n",
      "14 23.301980838508364 66.86456901777618 111.61415100097656\n",
      "15 20.32818640773533 68.54385517947541 0.13884620368480682\n",
      "16 0 70.1466238599872 4.306094646453857\n",
      "17 0 28.947309634941547 0.052763402462005615\n",
      "18 0 64.80600095038541 2.8415889739990234\n",
      "19 0 66.88485782475665 0.9111777544021606\n",
      "20 0 65.50301603038223 19.368316650390625\n",
      "21 17.068660808019594 68.72907991902578 0.052165452390909195\n",
      "22 0 69.4496925787772 0.548751711845398\n",
      "23 18.05219414353913 57.50252254999466 0.12847790122032166\n",
      "24 0 64.74977983742586 2.258470058441162\n",
      "25 0 84.8668197332506 13.074732780456543\n",
      "26 18.87381095282398 95.12734501484671 0.2933685779571533\n",
      "27 0 69.3474323451346 1.5195038318634033\n",
      "28 0 65.9832448338153 0.6068355441093445\n",
      "29 17.381032718461125 66.13427569554062 0.8618180751800537\n",
      "30 18.317377914998804 66.9188200318233 14.51422119140625\n",
      "31 42.22616287146793 124.97267678054453 0.039602190256118774\n",
      "32 35.735674437332335 104.55650012747978 0.05481204390525818\n",
      "33 21.16592543426699 147.52725035449927 0.020961247384548187\n",
      "34 42.22616287146793 91.68811400403021 0.7320588827133179\n",
      "35 37.84243205726038 96.56844154558287 0.006072592921555042\n",
      "36 40.554869874580305 115.98356763636568 1.1919059753417969\n",
      "37 0 64.07200785059602 0.1827758252620697\n",
      "38 19.602795037868628 110.37592982529212 4.628401756286621\n",
      "39 37.84243205726038 140.27466309970274 0.006920428480952978\n",
      "40 37.84243205726038 89.93379923981512 0.6510609984397888\n",
      "41 18.96857141452895 67.68121290709442 0.6754213571548462\n",
      "42 37.84243205726038 109.77448932603883 3.9767656326293945\n",
      "43 37.84243205726038 71.99637546958886 2.4697885513305664\n",
      "44 29.35832433052454 61.05303477133181 0.12009682506322861\n",
      "45 19.602795037868628 117.4901310671689 0.03379075601696968\n",
      "46 34.55394716856624 104.05044188926973 0.10592684149742126\n",
      "47 35.735674437332335 97.68569106172727 0.7479591369628906\n",
      "48 21.16592543426699 57.13756648632284 0.0009281404782086611\n",
      "49 37.84243205726038 97.35688530716952 0.04450520873069763\n",
      "50 37.84243205726038 106.33290890453956 0.0024842401035130024\n",
      "51 0 59.66895169626169 1.509310245513916\n",
      "52 40.554869874580305 98.80769985041192 0.0996330976486206\n",
      "53 42.22616287146793 111.44914102765414 0.5460429191589355\n",
      "54 17.468298331612985 131.22784888991623 0.1763041466474533\n",
      "55 0 0 0.0003807491739280522\n",
      "56 35.735674437332335 106.16643563239025 0.45101651549339294\n",
      "57 21.16592543426699 95.98299208892303 2.085583209991455\n",
      "58 37.84243205726038 135.45468736574617 0.009661166928708553\n",
      "59 17.068660808019594 82.73327560638681 0.3681240975856781\n",
      "60 20.32818640773533 110.10357202339813 0.6792528033256531\n",
      "61 42.22616287146793 95.66844051946087 0.4991108179092407\n",
      "62 39.10771094075316 104.1639087917328 0.47981926798820496\n",
      "63 37.84243205726038 97.79040521068369 4.533259391784668\n",
      "64 41.501137148573406 101.30569149098528 1.281042218208313\n",
      "65 43.17243014546103 103.58804489897877 0.14335010945796967\n",
      "66 21.16592543426699 140.63469527646626 0.5991032123565674\n",
      "67 37.84243205726038 97.90546297771799 6.104380130767822\n",
      "68 33.64885572897545 98.44775914694321 3.491077423095703\n",
      "69 0 104.94306261967517 0.14182870090007782\n",
      "70 22.144303357250518 123.58260764680638 0.3771352767944336\n",
      "71 37.84243205726038 91.08298017932744 1.6616721153259277\n",
      "72 0 88.33170414045605 1.2072246074676514\n",
      "73 31.120477910591596 99.10183743901669 0.17415133118629456\n",
      "74 40.554869874580305 106.98914901056406 0.02067229151725769\n",
      "75 22.144303357250518 129.4276834881209 0.04875507205724716\n",
      "76 39.10771094075316 99.53975391768336 1.593095302581787\n",
      "77 21.16592543426699 108.101990432511 0.6962592005729675\n",
      "78 23.301980838508364 103.7802341165975 3.2401375770568848\n",
      "79 33.17315911512123 106.14585315519172 0.006493404973298311\n",
      "80 0 105.53464569318629 0.16416439414024353\n",
      "81 40.554869874580305 137.2981862581425 0.10443204641342163\n",
      "82 37.84243205726038 90.25570288551545 0.03554229438304901\n",
      "83 39.10771094075316 127.64973461750388 0.023658938705921173\n",
      "84 0 58.13482523224692 0.0007430457626469433\n",
      "85 35.735674437332335 97.12446582225326 0.7598897814750671\n",
      "86 0 68.59015329351233 0.28884878754615784\n",
      "87 34.052102144473864 87.97362456954149 0.025078119710087776\n",
      "88 21.16592543426699 140.8089826956049 0.006773155648261309\n",
      "89 19.602795037868628 125.23703190936504 0.10602432489395142\n",
      "90 36.726772277058046 106.99105155674891 0.30379629135131836\n",
      "91 0 0 1.9287481336505152e-05\n",
      "92 37.84243205726038 99.31752669062065 0.8875071406364441\n",
      "93 37.84243205726038 105.15311115616176 0.04004039615392685\n",
      "94 0 67.36228215010846 3.1930315494537354\n",
      "95 36.726772277058046 71.48513661679542 0.14059539139270782\n",
      "96 17.912554856724064 107.72254909291183 0.1696270853281021\n",
      "97 34.84938073998157 99.55902105268787 0.02187676914036274\n",
      "98 18.96857141452895 99.80250324243069 2.117234230041504\n",
      "99 21.16592543426699 146.62712861252288 0.36126166582107544\n",
      "100 17.912554856724064 96.44702009282727 0.42923587560653687\n",
      "101 36.726772277058046 122.55825761701779 0.6562784910202026\n",
      "102 0 60.342459406224975 1.7095831632614136\n",
      "103 22.144303357250518 132.91368134544464 1.127724051475525\n",
      "104 37.84243205726038 132.3641424444258 8.02202033996582\n",
      "105 40.554869874580305 132.02512185504364 1.0974398851394653\n",
      "106 0 96.78949662205902 0.4638730585575104\n",
      "107 19.602795037868628 101.21414376199907 2.584965467453003\n",
      "108 18.96857141452895 143.71059148437732 1.0528895854949951\n",
      "109 35.735674437332335 96.41129828373131 0.2414812296628952\n",
      "110 0 27.763777961699756 5.293302092468366e-05\n",
      "111 18.96857141452895 95.93850885688461 3.71830153465271\n",
      "112 42.22616287146793 71.8060959761481 4.84515905380249\n",
      "113 19.602795037868628 98.3881563392074 0.0037970198318362236\n",
      "114 34.84938073998157 99.84471037413395 0.1667577475309372\n",
      "115 0 102.4167608239929 6.051009654998779\n",
      "116 19.602795037868628 97.86749529725674 29.908987045288086\n",
      "117 35.735674437332335 100.72354321381746 0.1306343376636505\n",
      "118 19.602795037868628 69.84012529682164 0.856128454208374\n",
      "119 36.726772277058046 103.01781578345066 0.2151884138584137\n",
      "120 22.144303357250518 100.87615749501595 0.08564474433660507\n",
      "121 61.936689657639896 111.86132618122895 0.942550003528595\n",
      "122 40.554869874580305 125.62022045314481 0.2596445679664612\n",
      "123 23.301980838508364 96.93683408604458 2.316728115081787\n",
      "124 36.726772277058046 103.57084350771667 2.3313682079315186\n",
      "125 0 50.59197192672447 0.24557237327098846\n",
      "126 33.18267272699223 96.17367441517621 22.136899948120117\n",
      "127 33.64885572897545 126.33822433390523 1.676448106765747\n",
      "128 18.96857141452895 100.90052011358335 0.05634734034538269\n",
      "129 34.16314248466747 98.4337947782891 38.451194763183594\n",
      "130 21.16592543426699 112.49673602678548 0.9581236243247986\n",
      "131 40.554869874580305 99.59433152823722 0.08813782036304474\n",
      "132 35.735674437332335 110.74508085044447 1.0230251550674438\n",
      "133 22.144303357250518 67.37062980850597 1.2782281637191772\n",
      "134 88.29150065348338 168.3928906172859 0.031230537220835686\n",
      "135 57.98444573830021 169.29453935228238 0.8322832584381104\n",
      "136 45.348216870653715 141.89042758050454 0.0649232342839241\n",
      "137 66.12144961113864 173.85659721870655 0.3534233868122101\n",
      "138 55.770373293530334 168.57896869043904 0.08486387878656387\n",
      "139 19.602795037868628 113.96252595411241 1.1188448667526245\n",
      "140 26.988403180130366 135.07429878853137 0.3116820752620697\n",
      "141 57.353872000556905 170.40079751980792 0.0760340616106987\n",
      "142 22.03367803052398 84.94076945009225 0.2730204164981842\n",
      "143 36.7444337413946 93.54722995477823 0.08570710569620132\n",
      "144 80.22397139707348 153.85626334772692 10.410906791687012\n",
      "145 64.95374761193179 115.54295031243369 0.48169463872909546\n",
      "146 62.882956931633 133.07037804932108 0.042080797255039215\n",
      "147 40.24752721219355 153.775457791983 12.174798965454102\n",
      "148 55.499535267896334 141.34267909218607 0.7978161573410034\n",
      "149 64.95374761193179 141.25802288323936 0.21301886439323425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13282, 13292, 13285, 13293, 13297, 13294, 13269, 13283, 13289, 13286]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr('как ставить значок на клавиатура', 138)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rP9cLU1Sg3jo"
   },
   "outputs": [],
   "source": [
    "ranger = PavlovRanker(inverse_index, word_count, forward_index, model)\n",
    "with open('./gdrive/My Drive/queries.txt', 'r') as fr:\n",
    "    with open('./result.csv', 'w') as fw: \n",
    "        fw.write(\"QueryId,DocumentId\\n\")\n",
    "            for idx, line in enumerate(fr):\n",
    "                \n",
    "\n",
    "                query_id, query = int(line.split()[0]), ' '.join(line.split()[1:])\n",
    "                try:\n",
    "                    result = ranger(query, query_id)\n",
    "                except:\n",
    "                    print(\"exception occured in \", query_id)\n",
    "\n",
    "                for docID in result:\n",
    "                    fw.write(\"{},{}\\n\".format(query_id, docID))\n",
    "\n",
    "\n",
    "\n",
    "def get_dict(filename)\n",
    "    finded = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            q_id, doc_id = line.split()\n",
    "            q_id = int(q_id)\n",
    "            doc_id = int(doc_id.rstrip())\n",
    "            if (q_id not in finded):\n",
    "                finded[q_id] = [doc_id]\n",
    "            else:\n",
    "                finded[q_id].append(doc_id)\n",
    "\n",
    "        return finded\n",
    "\n",
    "finded = get_dict('./gdrive/My Drive/finded.csv')\n",
    "ranked = get_dict('/gdrive/My Drive/result.csv')\n",
    "\n",
    "with open('try10.csv', 'w') as f:\n",
    "    f.write('QueryId,DocumentId')\n",
    "    for key in sorted(ranked.keys()):\n",
    "        i = 0\n",
    "        writed = []\n",
    "        if (key in finded):\n",
    "            \n",
    "            for elem in finded[key]:\n",
    "                i += 1\n",
    "                f.write('{},{}\\n'.foramt(key, elem))\n",
    "                writed.append(elem)\n",
    "\n",
    "        while (i < 10):\n",
    "            for elem in ranked[key]:\n",
    "                if (elem not in writed):\n",
    "                    i += 1\n",
    "                    f.write('{},{}\\n'.foramt(key, elem))\n",
    "                if (i == 10):\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "pKxfOBri4XFS",
    "outputId": "2a06c2f3-dcc5-450e-f723-75fce2b75095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-19 18:50:46--  http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_lemmatize/ft_native_300_ru_wiki_lenta_lemmatize.vec\n",
      "Resolving files.deeppavlov.ai (files.deeppavlov.ai)... 93.175.29.74\n",
      "Connecting to files.deeppavlov.ai (files.deeppavlov.ai)|93.175.29.74|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2581071412 (2.4G) [application/octet-stream]\n",
      "Saving to: ‘ft_native_300_ru_wiki_lenta_lemmatize.vec’\n",
      "\n",
      "mmatize.vec          76%[==============>     ]   1.85G  5.27MB/s    eta 73s    ^C\n"
     ]
    }
   ],
   "source": [
    "!wget http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_lemmatize/ft_native_300_ru_wiki_lenta_lemmatize.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pzqYUdtSgbH9"
   },
   "outputs": [],
   "source": [
    "#try use word2vec, but it works bad\n",
    "class BMWord2vec:\n",
    "\n",
    "    def __init__(self, inverse_index, words_idf ,forward_index, word2vec):\n",
    "        self.inverse_index = inverse_index\n",
    "        self.word_count = word_count\n",
    "        self.forward_index = forward_index\n",
    "        self.words_idf = words_idf\n",
    "        self.word2vec = word2vec\n",
    "\n",
    "    def get_important(self, words, words_idf, threshold = 0.9):\n",
    "\n",
    "\n",
    "        if (len(words) <=  1):\n",
    "            return words\n",
    "\n",
    "        \n",
    "        variants = [words]\n",
    "\n",
    "        for variant in variants:\n",
    "            total_idf = sum([self.words_idf[word] for word in variant])\n",
    "            for idx, word in enumerate(variant):\n",
    "                probable = variant[:idx] + variant[idx+1:]\n",
    "                if ((total_idf - self.words_idf[word]) / total_idf > threshold and probable not in variants):\n",
    "                    variants.append(probable)\n",
    "                    \n",
    "        return variants\n",
    "         \n",
    "    def computeScore(self, query_words, text):\n",
    "        def computeCos(vec1, vec2):\n",
    "            vec1_norm = np.sqrt(np.sum(vec1 ** 2))\n",
    "            vec2_norm = np.sqrt(np.sum(vec2 ** 2))\n",
    "\n",
    "            cos = np.sum(vec1 * vec2) / (vec1_norm * vec2_norm)\n",
    "\n",
    "            return cos\n",
    "        \n",
    "        score = 0\n",
    "        text = text.split()\n",
    "        for word in query_words:\n",
    "            vector_exist = False\n",
    "            if word in self.word2vec:\n",
    "                word_vec = np.asarray(self.word2vec[word])\n",
    "                vector_exist = True\n",
    "\n",
    "            for text_word in text:\n",
    "                text_exist = False\n",
    "                if (text_word in self.word2vec):\n",
    "                    text_vec = np.asarray(self.word2vec[text_word])\n",
    "                    text_exist = True\n",
    "\n",
    "                if (text_exist and vector_exist and word in words_idf and text_word in words_idf):\n",
    "                    pair_score = computeCos(text_vec, word_vec)\n",
    "                    #print(pair_score, word, text_word)\n",
    "                    score += pair_score * min(self.words_idf[text_word], self.words_idf[word])\n",
    "\n",
    "                elif (text_word == word and word in words_idf):\n",
    "                    score += ( words_idf[word])\n",
    "        \n",
    "        score = score / (len(query_words) * len(text))\n",
    "\n",
    "        return score\n",
    "   \n",
    "\n",
    "\n",
    "    def __call__(self, query, q_id):\n",
    "\n",
    "\n",
    "        splitted = query.split()\n",
    "        words_idf = {}\n",
    "        words = []\n",
    "\n",
    "        for word in splitted:\n",
    "            if word in self.words_idf:\n",
    "                words.append(word)\n",
    "\n",
    "\n",
    "        all_docs = set()\n",
    "        threshold = 0.7\n",
    "\n",
    "        if (len(words) > 12):\n",
    "            wordidf = [(word, words_idf[word]) for word in words]\n",
    "            wordidf = sorted(wordidf, key=lambda x:x[1], reverse=True)[:12]\n",
    "            wordidf = [word[0] for word in wordidf]\n",
    "            words = [word for word in words if word in wordidf]\n",
    "\n",
    "        while (len(all_docs) < 10):\n",
    "            \n",
    "                print(threshold)\n",
    "                probable_queries = self.get_important(words, words_idf, threshold)\n",
    " \n",
    "                for probable_query in probable_queries:\n",
    "\n",
    "                    relevant = self.inverse_index[probable_query[0]]\n",
    "                    for word in probable_query[1:]:\n",
    "                        \n",
    "                        relevant = relevant.intersection(self.inverse_index[word])\n",
    "                    \n",
    "                    all_docs |= relevant\n",
    "                \n",
    "                threshold -= 0.1\n",
    "            \n",
    "                all_docs = set(\n",
    "                        [docID for docID in all_docs \n",
    "                        if (docID > 96*(q_id-1) - 50) and (docID < (96*q_id-1) + 150)]\n",
    "                        )\n",
    "                \n",
    "        docID_score = []\n",
    "        for docID in all_docs:\n",
    "            \n",
    "            all_text = self.forward_index[docID][0] + ' ' + self.forward_index[docID][-1]\n",
    "            score = self.computeScore(words, all_text)\n",
    "            docID_score.append((docID, score))\n",
    "            #print(docID, score)\n",
    "\n",
    "        docID_score = sorted(docID_score, key=lambda x:x[1], reverse=True)[:10]\n",
    "        docIDs = [elem[0] for elem in docID_score]\n",
    "\n",
    "        return docIDs\n",
    "\n",
    "\n",
    "def compute_words_idf(word_count):\n",
    "    total_words = sum([word_count[word] for word in word_count.keys()])\n",
    "    words_idf = {}\n",
    "    for word in word_count.keys():\n",
    "        words_idf[word] = np.log(total_words / word_count[word])\n",
    "\n",
    "    return words_idf\n",
    "\n",
    "def loadWord2vec(path):\n",
    "    with open(path, 'r') as f:\n",
    "        f.readline()\n",
    "        word2vec = {}\n",
    "        for line in f:\n",
    "            word, vector = line.split()[0], line.split()[1:]\n",
    "            vector = [float(val) for val in vector]\n",
    "            word2vec[word] = vector\n",
    "        return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMP-wNUrvG6l"
   },
   "outputs": [],
   "source": [
    "word2vecs = loadWord2vec('ft_native_300_ru_wiki_lenta_lemmatize.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SNv12L4xDsA"
   },
   "outputs": [],
   "source": [
    "words_idf = compute_words_idf(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFJi9pnUfnBm"
   },
   "outputs": [],
   "source": [
    "ranker = BMWord2vec(inverse_index, words_idf, forward_index, word2vecs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled5.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
